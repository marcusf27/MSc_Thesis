{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d8553d-d97f-40b5-b943-8f88e0ffbf49",
   "metadata": {},
   "source": [
    "Install Required Python Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee430777-eb65-494b-a6fa-907bd0182cce",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gitpython\n",
      "  Obtaining dependency information for gitpython from https://files.pythonhosted.org/packages/1d/9a/4114a9057db2f1462d5c8f8390ab7383925fe1ac012eaa42402ad65c2963/GitPython-3.1.44-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas in c:\\program files\\anaconda\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: openpyxl in c:\\program files\\anaconda\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: requests in c:\\program files\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/a0/61/5c78b91c3143ed5c14207f463aecfc8f9dbb5092fb2869baf37c273b2705/gitdb-4.0.12-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\anaconda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: et_xmlfile in c:\\program files\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\anaconda\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\anaconda\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\anaconda\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/04/be/d09147ad1ec7934636ad912901c5fd7667e1c858e19d355237db0d0cd5e4/smmap-5.0.2-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/207.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  204.8/207.6 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 207.6/207.6 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB ? eta 0:00:00\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, gitdb, gitpython\n",
      "Successfully installed gitdb-4.0.12 gitpython-3.1.44 smmap-5.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gitpython pandas openpyxl requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb9d5c1-b9c9-4568-8019-9bb6ba277d6e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "#repo_url = \"https://github.com/elastic/elasticsearch\"  # Replace with your repo URL\n",
    "repo_url = \"https://github.com/marcusf27/test_master\"\n",
    "#\"freeCodeCamp\": \"https://github.com/freeCodeCamp/freeCodeCamp\",\n",
    "#repo_dir = \"elasticsearch\"  # Directory to clone the repository\n",
    "repo_dir = \"test_master\"\n",
    "\n",
    "project_type = \"gradle\"\n",
    "build_agent=\"maven:3.6.3-jdk-11\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4678138-d28a-4411-9dc4-0aed5a97ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = repo_dir\n",
    "#SonarQube \n",
    "#sonar_host = \"http://localhost:9000\"  # SonarQube URL\n",
    "#sonar_token = \"\"  # Replace with your token\n",
    "project_key = repo_dir\n",
    "#Codescene\n",
    "#\"http://localhost:3003/api/v2\n",
    "codescene_url = \"http://localhost:3003/api/v2\"\n",
    "#codescene_token = \"eyJpZCI6MiwicmFuZCI6InhURjhwTHpkWDJXR25aVWhFMlREVStLSlArUjRmRE5rIn0\"  # Replace with your token\n",
    "codescene_token = \"eyJpZCI6MywicmFuZCI6InlaR2ozQWxIV05KQ3BxZ3hSNFpoZUFZRnhxRU53TUtXIn0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3335076d-8700-4ce2-a0a8-a151dedbc2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists at test_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "\n",
    "# Check if the repository already exists\n",
    "if not os.path.exists(repo_dir):\n",
    "    # Clone the repository if the directory doesn't exist\n",
    "    Repo.clone_from(repo_url, repo_dir)\n",
    "    print(f\"Repository cloned to {repo_dir}\")\n",
    "else:\n",
    "    print(f\"Repository already exists at {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "040f4af1-cac8-4ce6-84dd-3ece62fc21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project type: unknown\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def recognize_project_type(project_path):\n",
    "    if os.path.exists(os.path.join(project_path, 'pom.xml')):\n",
    "        return 'maven'\n",
    "    elif os.path.exists(os.path.join(project_path, 'build.gradle')):\n",
    "        return 'gradle'\n",
    "    elif any(file.endswith('.csproj') for file in os.listdir(project_path)):\n",
    "        return 'dotnet'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "project_type = recognize_project_type(repo_dir)\n",
    "#project_type = 'java'\n",
    "print(f'Project type: {project_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "487d3f12-c2ef-47f9-a5e5-202f7e3886c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown project type\n",
      "Container ID: None\n"
     ]
    }
   ],
   "source": [
    "container_id = None\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "project_dir = os.path.join(current_dir, repo_dir)\n",
    "\n",
    "if project_type == 'maven':\n",
    "    container_id = os.popen(f'docker run -d -v {project_dir}:/usr/src/mymaven -w /usr/src/mymaven {build_agent} mvn clean install').read().strip()\n",
    "elif project_type == 'gradle':\n",
    "    container_id = os.popen(f'docker run -d -v {project_dir}:/home/gradle/project -w /home/gradle/project gradle:latest gradle build --debug').read().strip()\n",
    "elif project_type == 'dotnet':\n",
    "    if build_version:\n",
    "        container_id = os.popen(f'docker run -d -v {project_dir}:/app -w /app mcr.microsoft.com/dotnet/sdk:{dotnet_version} dotnet build').read().strip()\n",
    "    else:\n",
    "        container_id = os.popen(f'docker run -d -v {project_dir}:/app -w /app mcr.microsoft.com/dotnet/sdk:6.0 dotnet build').read().strip()\n",
    "else:\n",
    "    print('Unknown project type')\n",
    "\n",
    "print(f'Container ID: {container_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d971bf-7015-4408-9423-dc44a024bc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3578959190.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    container_id = osmaven:3.6.3-jdk-11popen(f'docker run -d -v {project_dir}:/usr/src/mymaven -w /usr/src/mymaven {build_agent} mvn clean install').read().strip()\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "container_id = None\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "project_dir = os.path.join(current_dir, repo_dir)\n",
    "\n",
    "if project_type == 'maven':\n",
    "    container_id = osmaven:3.6.3-jdk-11popen(f'docker run -d -v {project_dir}:/usr/src/mymaven -w /usr/src/mymaven {build_agent} mvn clean install').read().strip()\n",
    "elif project_type == 'gradle':\n",
    "    container_id = os.popen(f'docker run -d -v {project_dir}:/home/gradle/project -w /home/gradle/project gradle:7.2-jdk11 gradle build --debug').read().strip()\n",
    "elif project_type == 'dotnet':\n",
    "    if dotnet_version:\n",
    "        container_id = os.popen(f'docker run -d -v {project_dir}:/app -w /app mcr.microsoft.com/dotnet/sdk:{dotnet_version} dotnet build').read().strip()\n",
    "    else:\n",
    "        container_id = os.popen(f'docker run -d -v {project_dir}:/app -w /app mcr.microsoft.com/dotnet/sdk:6.0 dotnet build').read().strip()\n",
    "else:\n",
    "    print('Unknown project type')\n",
    "\n",
    "print(f'Container ID: {container_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cd154f-01f4-429d-a26c-768120780d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n",
      "Checking build\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Check every 5 seconds\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container_id\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time  # Add this import\n",
    "\n",
    "# Wait for the container to finish\n",
    "while True:\n",
    "    print(\"Checking build\")\n",
    "    status = os.popen(f'docker inspect -f {{.State.Running}} {container_id}').read().strip()\n",
    "    print(status)\n",
    "    if status == 'false':\n",
    "        break\n",
    "    time.sleep(5)  # Check every 5 seconds\n",
    "\n",
    "return container_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf48d138-5f43-4fad-a209-44cfc4ebbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131f4f8-9695-4228-b182-8c7fbaf7c019",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e50b9-6133-4737-bca3-68cb9a786bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa3e840e-925a-47a1-9ef2-a34b5653f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: test_master\n",
      "[{'id': 1, 'name': 'test_master', 'ref': '/api/v2/projects/1'}]\n",
      "Project 'test_master' already exists with ID 1.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {codescene_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "#print(repo_url)\n",
    "print(\"Project name:\", project_name)\n",
    "\n",
    "# Check if project already exists\n",
    "projects_response = requests.get(f\"{codescene_url}/projects\", headers=headers)\n",
    "\n",
    "if projects_response.status_code == 200:\n",
    "    projects = projects_response.json()[\"projects\"]\n",
    "    existing_project = next((p for p in projects if p[\"name\"] == project_name), None)\n",
    "    print(projects)\n",
    "    if existing_project:\n",
    "        print(f\"Project '{project_name}' already exists with ID {existing_project['id']}.\")\n",
    "        project_id = existing_project['id']\n",
    "    else:\n",
    "        # Create the project if it doesn't exist\n",
    "        payload = {\n",
    "            \"name\": project_name,\n",
    "             \"config\": \n",
    "                {\n",
    "                    \"repo-paths\": [repo_url],\n",
    "                    \"type\": \"git\",\n",
    "                }\n",
    "            \n",
    "        }\n",
    "        response = requests.post(\n",
    "            f\"{codescene_url}/projects/new\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "\n",
    "        if response.status_code == 201:\n",
    "            print(f\"Project '{project_name}' created successfully.\")\n",
    "            project_id = response.json().get(\"id\")\n",
    "            print(f\"Project ID: {project_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to create project: {response.status_code}\")\n",
    "            print(response.json())\n",
    "else:\n",
    "    print(f\"Failed to retrieve projects: {projects_response.status_code}\")\n",
    "    print(projects_response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468a3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers2 = {\n",
    "    \"Authorization\": f\"Bearer {codescene_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9bee47-6e2a-40d5-b1de-67469589910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing analysis found with ID: 12\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {codescene_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.get(f\"{codescene_url}/projects/{project_id}/analyses/latest\", headers=headers)\n",
    "\n",
    "if response.status_code == 404:\n",
    "    print(\"No existing analysis found.\")\n",
    "    analysis_id = 0\n",
    "else:    \n",
    "    response.raise_for_status()\n",
    "    analysis = response.json()\n",
    "    print(f\"Existing analysis found with ID: {analysis['id']}\")\n",
    "    analysis_id = analysis[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c7b80f7-badc-41e7-aeff-974812fa9ef7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 1, 'max_pages': 1, 'analyses': [{'id': 12, 'name': 'test_master', 'analysistime': '2025-02-04T12:14:16Z', 'ref': '/api/v2/projects/1/analyses/12'}, {'id': 11, 'name': 'test_master', 'analysistime': '2025-02-04T08:33:38Z', 'ref': '/api/v2/projects/1/analyses/11'}, {'id': 10, 'name': 'test_master', 'analysistime': '2025-02-04T08:31:48Z', 'ref': '/api/v2/projects/1/analyses/10'}, {'id': 9, 'name': 'test_master', 'analysistime': '2025-02-03T08:50:55Z', 'ref': '/api/v2/projects/1/analyses/9'}, {'id': 8, 'name': 'test_master', 'analysistime': '2025-01-29T08:59:22Z', 'ref': '/api/v2/projects/1/analyses/8'}, {'id': 7, 'name': 'test_master', 'analysistime': '2025-01-29T08:58:48Z', 'ref': '/api/v2/projects/1/analyses/7'}, {'id': 6, 'name': 'test_master', 'analysistime': '2025-01-29T08:16:54Z', 'ref': '/api/v2/projects/1/analyses/6'}, {'id': 5, 'name': 'test_master', 'analysistime': '2025-01-28T13:07:19Z', 'ref': '/api/v2/projects/1/analyses/5'}, {'id': 4, 'name': 'test_master', 'analysistime': '2025-01-28T08:06:52Z', 'ref': '/api/v2/projects/1/analyses/4'}, {'id': 3, 'name': 'test_master', 'analysistime': '2025-01-28T08:03:13Z', 'ref': '/api/v2/projects/1/analyses/3'}, {'id': 2, 'name': 'test_master', 'analysistime': '2025-01-27T12:29:46Z', 'ref': '/api/v2/projects/1/analyses/2'}, {'id': 1, 'name': 'test_master', 'analysistime': '2025-01-27T12:08:20Z', 'ref': '/api/v2/projects/1/analyses/1'}]}\n",
      "Latest analysis time is for project 1 is with id 12 and time 2025-02-04T12:14:16Z\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f\"{codescene_url}/projects/{project_id}/analyses\", headers=headers)\n",
    "analyses = response.json()\n",
    "\n",
    "print(analyses)\n",
    "\n",
    "# Get the latest analysis by sorting based on analysis time\n",
    "\n",
    "if analyses.get('analyses'):\n",
    "    latest_analysis = sorted(analyses['analyses'], key=lambda x: x['analysistime'], reverse=True)[0]\n",
    "    analysis_time = latest_analysis['analysistime']\n",
    "    print(f\"Latest analysis time is for project {project_id} is with id {latest_analysis['id']} and time {analysis_time}\")\n",
    "else:\n",
    "    analysis_time = \"1970-01-01T00:00:00Z\"  # Default date as the earliest possible\n",
    "    print(f\"Latest analysis time is for project {project_id} does not exists. Indicates new project!\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c73238c-aa9e-4d11-a096-c02e4625ba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis started successfully.\n",
      "{'ok': 'analysis-scheduled'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {codescene_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    #\"present-x-ray-overloads-separately\": \"True\",\n",
    "}\n",
    "\n",
    "# Trigger an analysis for the project\n",
    "analysis_response = requests.post(f\"{codescene_url}/projects/{project_id}/run-analysis\", headers=headers)\n",
    "\n",
    "if analysis_response.status_code == 202:\n",
    "    print(\"Analysis started successfully.\")\n",
    "    print(analysis_response.json())\n",
    "    analysis_id = analysis_id +1\n",
    "else:\n",
    "    print(f\"Failed to start analysis: {analysis_response.json()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8908a782-9d60-4fb2-9518-31286204f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking analysis with ID 13 for project 1\n",
      "Waiting for analysis to complete...\n",
      "[{'id': 12, 'name': 'test_master', 'analysistime': '2025-02-04T12:14:16Z', 'ref': '/api/v2/projects/1/analyses/12'}, {'id': 11, 'name': 'test_master', 'analysistime': '2025-02-04T08:33:38Z', 'ref': '/api/v2/projects/1/analyses/11'}, {'id': 10, 'name': 'test_master', 'analysistime': '2025-02-04T08:31:48Z', 'ref': '/api/v2/projects/1/analyses/10'}, {'id': 9, 'name': 'test_master', 'analysistime': '2025-02-03T08:50:55Z', 'ref': '/api/v2/projects/1/analyses/9'}, {'id': 8, 'name': 'test_master', 'analysistime': '2025-01-29T08:59:22Z', 'ref': '/api/v2/projects/1/analyses/8'}, {'id': 7, 'name': 'test_master', 'analysistime': '2025-01-29T08:58:48Z', 'ref': '/api/v2/projects/1/analyses/7'}, {'id': 6, 'name': 'test_master', 'analysistime': '2025-01-29T08:16:54Z', 'ref': '/api/v2/projects/1/analyses/6'}, {'id': 5, 'name': 'test_master', 'analysistime': '2025-01-28T13:07:19Z', 'ref': '/api/v2/projects/1/analyses/5'}, {'id': 4, 'name': 'test_master', 'analysistime': '2025-01-28T08:06:52Z', 'ref': '/api/v2/projects/1/analyses/4'}, {'id': 3, 'name': 'test_master', 'analysistime': '2025-01-28T08:03:13Z', 'ref': '/api/v2/projects/1/analyses/3'}, {'id': 2, 'name': 'test_master', 'analysistime': '2025-01-27T12:29:46Z', 'ref': '/api/v2/projects/1/analyses/2'}, {'id': 1, 'name': 'test_master', 'analysistime': '2025-01-27T12:08:20Z', 'ref': '/api/v2/projects/1/analyses/1'}]\n",
      "[{'id': 13, 'name': 'test_master', 'analysistime': '2025-02-19T09:40:52Z', 'ref': '/api/v2/projects/1/analyses/13'}, {'id': 12, 'name': 'test_master', 'analysistime': '2025-02-04T12:14:16Z', 'ref': '/api/v2/projects/1/analyses/12'}, {'id': 11, 'name': 'test_master', 'analysistime': '2025-02-04T08:33:38Z', 'ref': '/api/v2/projects/1/analyses/11'}, {'id': 10, 'name': 'test_master', 'analysistime': '2025-02-04T08:31:48Z', 'ref': '/api/v2/projects/1/analyses/10'}, {'id': 9, 'name': 'test_master', 'analysistime': '2025-02-03T08:50:55Z', 'ref': '/api/v2/projects/1/analyses/9'}, {'id': 8, 'name': 'test_master', 'analysistime': '2025-01-29T08:59:22Z', 'ref': '/api/v2/projects/1/analyses/8'}, {'id': 7, 'name': 'test_master', 'analysistime': '2025-01-29T08:58:48Z', 'ref': '/api/v2/projects/1/analyses/7'}, {'id': 6, 'name': 'test_master', 'analysistime': '2025-01-29T08:16:54Z', 'ref': '/api/v2/projects/1/analyses/6'}, {'id': 5, 'name': 'test_master', 'analysistime': '2025-01-28T13:07:19Z', 'ref': '/api/v2/projects/1/analyses/5'}, {'id': 4, 'name': 'test_master', 'analysistime': '2025-01-28T08:06:52Z', 'ref': '/api/v2/projects/1/analyses/4'}, {'id': 3, 'name': 'test_master', 'analysistime': '2025-01-28T08:03:13Z', 'ref': '/api/v2/projects/1/analyses/3'}, {'id': 2, 'name': 'test_master', 'analysistime': '2025-01-27T12:29:46Z', 'ref': '/api/v2/projects/1/analyses/2'}, {'id': 1, 'name': 'test_master', 'analysistime': '2025-01-27T12:08:20Z', 'ref': '/api/v2/projects/1/analyses/1'}]\n",
      "Analysis completed successfully. New analysis is 13\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    " \n",
    "timeout=6000\n",
    "\n",
    "existing_analysis_time = datetime.strptime(analysis_time, \"%Y-%m-%dT%H:%M:%SZ\")  # Format of CodeScene analysis time\n",
    "\n",
    "print(f\"Checking analysis with ID {analysis_id} for project {project_id}\")\n",
    "# Poll for analysis completion\n",
    "print(\"Waiting for analysis to complete...\")\n",
    "for _ in range(timeout // 10):  # Poll every 10 seconds\n",
    "    \n",
    "    response = requests.get(f\"{codescene_url}/projects/{project_id}/analyses\", headers=headers)\n",
    "    #print(response.json())\n",
    "    if response.status_code == 200:\n",
    "        analysis = response.json()['analyses']\n",
    "        if analysis:\n",
    "            print(analysis)\n",
    "            latest_analysis = sorted(analysis, key=lambda x: x['analysistime'], reverse=True)[0]\n",
    "            new_analysis_time  = latest_analysis['analysistime']\n",
    "            new_analysis_time = datetime.strptime(new_analysis_time , \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "            if new_analysis_time > existing_analysis_time:\n",
    "                print(f\"Analysis completed successfully. New analysis is {latest_analysis['id']}\")\n",
    "                analysis_id = latest_analysis['id']\n",
    "                break\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    raise TimeoutError(\"Analysis did not complete within the timeout period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0036f330-e5c8-4194-a17e-37a28aa72e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching analyses for project: test_master (ID: 1)\n",
      "Analysis results saved to CodeScene_test_master_ID_1_files.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# API headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {codescene_token}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "codescene_file_name = f\"CodeScene_{project_name}_ID_{project_id}_files.csv\".replace(\" \", \"_\")\n",
    "page_size = 50  # Number of results per page\n",
    "order_by = \"code_health\"  # Sorting parameter (e.g., complexity, churn)\n",
    "\n",
    "print(f\"\\nFetching analyses for project: {project_name} (ID: {project_id})\")\n",
    "\n",
    "# Fetch paginated results\n",
    "page = 1\n",
    "all_files = []\n",
    "endpoint2 = f\"{codescene_url}/projects/{project_id}/analyses/2/code\"\n",
    "while True:\n",
    "    # API endpoint with pagination\n",
    "    endpoint = f\"{codescene_url}/projects/{project_id}/analyses/latest/files\"\n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"page_size\": page_size,\n",
    "        \"order_by\": order_by,\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        files = data.get(\"files\", [])\n",
    "\n",
    "        # Add files to the result list\n",
    "        all_files.extend(files)\n",
    "\n",
    "        # Check if there are more pages\n",
    "        if len(files) < page_size:\n",
    "            break  # No more data to fetch\n",
    "        else:\n",
    "            page += 1  # Move to the next page\n",
    "    else:\n",
    "        print(f\"Failed to fetch results: {response.status_code}\")\n",
    "        print(response.json())\n",
    "        break\n",
    "\n",
    "# Convert the results to a DataFrame and export to CSV\n",
    "#print(all_files)\n",
    "if all_files:\n",
    "    df = pd.DataFrame(all_files)\n",
    "    output_file = f\"CodeScene_{project_name}_ID_{project_id}_files.csv\".replace(\" \", \"_\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Analysis results saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No results found.\")\n",
    "    \n",
    "    \n",
    "codescene_df = pd.read_csv(codescene_file_name)\n",
    "\n",
    "output_file = f\"{codescene_df.to_csv}_Analysis_Results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8d7849f-5b4b-4e43-a79c-b02853ff583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined analysis results exported to test_master_Analysis_Results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "output_file = f\"{project_name}_Analysis_Results.csv\"\n",
    "\n",
    "# Load SonarQube data\n",
    "#sonarqube_df = pd.read_csv(sonarqube_file)\n",
    "#sonarqube_df[\"component\"] = sonarqube_df[\"component\"].apply(\n",
    "#    lambda x: x.replace(f\"{project_key}:{project_key}\", project_name) if x.startswith(f\"{project_key}:{project_key}\") else x\n",
    "#)\n",
    "# Clean and preprocess SonarQube data\n",
    "#sonarqube_df[\"vulnerabilityProbability\"] = pd.to_numeric(sonarqube_df[\"vulnerabilityProbability\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Process SonarQube data: Group by file and calculate number of vulnerabilities and average probability\n",
    "#sonarqube_processed = (\n",
    "#    sonarqube_df.groupby(\"component\", as_index=False)\n",
    "#    .agg(\n",
    "#        num_vulnerabilities=(\"key\", \"count\"),  # Count vulnerabilities per file\n",
    "#        vulnerability_probability=(\"vulnerabilityProbability\", \"mean\"),  # Average probability\n",
    "#    )\n",
    "#    .rename(columns={\"component\": \"path\"})  # Rename 'component' to 'file' for merging\n",
    "#)\n",
    "#print(sonarqube_processed)\n",
    "# Load CodeScene data\n",
    "codescene_df = pd.read_csv(codescene_file_name)\n",
    "\n",
    "# Process CodeScene data: Rename columns to match\n",
    "#codescene_processed = codescene_df[[\"file\", \"issues\"]].rename(columns={\"issues\": \"num_issues\"})\n",
    "\n",
    "# Combine the two datasets\n",
    "#combined_df = pd.merge(sonarqube_processed, codescene_df, on=\"path\", how=\"inner\")\n",
    "\n",
    "# Fill missing values with 0 for files not present in one of the datasets\n",
    "#combined_df.fillna(0, inplace=True)\n",
    "\n",
    "# Export the combined data\n",
    "#combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined analysis results exported to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de6f50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def load_csv(file_path):\n",
    "    try:\n",
    "        #print(\"jag kan läsa fil\")\n",
    "        return pd.read_csv(file_path)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"File was not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f3b226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Constructor Over-Injection': 1, 'Complex Conditional': 3, 'Complex Method': 3, 'Bumpy Road': 2, 'Large Method': 1, 'Deep, Nested Complexity': 2, 'Excess Number of Function Arguments': 1, 'High Degree of Code Duplication': 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "input_file = 'CodeScene_test_master_ID_1_files.csv'\n",
    "\n",
    "matched_files = \"smells.csv\"\n",
    "\n",
    "searching_columns = [\"recommendations\"]\n",
    "values_to_search = [\"Low Overall Code Complexity\", \n",
    "                    \"Constructor Over-Injection\", \n",
    "                    \"Complex Conditional\", \n",
    "                    \"Complex Method\", \n",
    "                    \"Large Method\", \n",
    "                    \"Bumpy Road\",\n",
    "                    \"Deep, Nested Complexity\",\n",
    "                    \"Excess Number of Function Arguments\",\n",
    "                    \"High Degree of Code Duplication\"]\n",
    "df = load_csv(input_file)\n",
    "\n",
    "data = df['recommendations']\n",
    "pattern = r\"'title': '(.*?)', 'severity'\"\n",
    "\n",
    "\n",
    "#Get the matches\n",
    "res = [match for val in data for match in re.findall(pattern, val)]\n",
    "\n",
    "#Exclude these\n",
    "excluded_phrases = {'Low Overall Code Complexity', 'Custom Rule: Detect TODOs'}\n",
    "\n",
    "#Add the rest\n",
    "filtered_res = [smell for smell in res if smell not in excluded_phrases]\n",
    "\n",
    "#Make it into a dictionary with smell-occurance.\n",
    "smell_counter = dict(Counter(filtered_res))\n",
    "\n",
    "print(smell_counter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eb1c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_command(command):\n",
    "    #Runs a command, and return the output from the terminal\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dfe72b8-1323-4f19-bcde-cb41b39eb502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while deleting the folder: [WinError 5] Åtkomst nekad: 'test_master\\\\.git\\\\objects\\\\pack\\\\pack-097104f0d2a6d4d48ac06a6925ca0cfb5d5a8fec.idx'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "# Clean up\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(repo_dir) and os.path.isdir(repo_dir):\n",
    "    try:\n",
    "        # Delete the folder and its contents\n",
    "        shutil.rmtree(repo_dir)\n",
    "        print(f\"Folder '{repo_dir}' has been deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting the folder: {e}\")\n",
    "else:\n",
    "    print(f\"Folder '{repo_dir}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8f91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
